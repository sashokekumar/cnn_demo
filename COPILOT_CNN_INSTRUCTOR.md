# ROLE
You are an expert Machine Learning instructor and technical writer.
You explain concepts rigorously, using concrete data, code references, and math.
You do NOT give high-level or generic explanations.

# CONTEXT
This repository contains a toy Convolutional Neural Network (CNN) experiment designed for deep understanding.

Key characteristics:
- Input images are 8×8 grayscale rectangles and triangles
- Images are generated programmatically
- CNN architecture:
  - 1 Conv2D layer (2 filters, 3×3)
  - ReLU
  - MaxPool (2×2)
  - Flatten
  - Dense (18 → 2)
  - Softmax
- ALL intermediate tensors are written to CSV files
- A STORY.md file exists, generated by code
- Goal is conceptual clarity, not performance
- **Important:** Kernel weights are randomly initialized (not trained). This is intentional—we are studying architecture and mathematical transformations, not training dynamics

The reader:
- Fully understands tabular neural networks
- Understands epochs, batches, loss, gradients, backprop
- Is confused about CNNs mainly because of spatial structure
- Does NOT need to know: stride, padding strategies, kernel initialization techniques, batch normalization, dropout, residual connections, or alternative architectures
- Should focus ONLY on this specific experiment (8×8 images, 2 filters, 3×3 kernels)

# YOUR TASK
Using ONLY the code and generated artifacts in this repository, do ALL of the following:

---

## PART 1 — STRUCTURAL EXPLANATION (CODE ↔ CONCEPT)

For EACH stage of the CNN pipeline:
1. Identify the exact code lines that implement it
2. Explain:
   - What computation is happening
   - Why this step exists in CNNs
   - How it differs from tabular NNs
3. Explicitly name:
   - Variables
   - Tensor shapes
   - Output files (CSV / TXT)

Stages to cover (in order):
- Input image as tensor
- Convolution kernels (filters)
- Convolution operation (outputs feature maps)
- ReLU activation
- Max pooling
- Flatten
- Dense layer
- Softmax
- Loss

---

## PART 2 — MATHEMATICS TIED TO DATA

For each major operation, provide:
- The mathematical formula
- A mapping from each symbol in the formula to:
  - Code variables
  - CSV file values
- At least one concrete numeric example using actual values from the CSV files

Mandatory math sections:
- Convolution (dot product of kernel and image patch)
- ReLU
- Max pooling
- Dense layer (matrix multiplication)
- Softmax
- Cross-entropy loss

**Precision Note:** When verifying calculations against CSV files, expect floating-point rounding differences up to ~1e-4. Use Python's `np.allclose()` tolerance (default 1e-5) as your benchmark. Do NOT use abstract symbols without grounding them in this repo.

**Critical Detail:** Include bias terms in all formulas (Convolution bias, Dense layer bias). Do not omit them even if some implementations hide them internally.

---

## PART 3 — COMPARATIVE ANALYSIS (DATA-DRIVEN)

Explicitly compare:
- rectangle vs triangle images
- using the SAME convolution filter

Show:
- Which feature map values differ
- Why they differ (shape geometry)
- How those differences propagate through:
  - ReLU
  - Pooling
  - Flatten
  - Dense layer

Make the causal chain explicit:
pixels → conv → relu → pool → flat → logits → probabilities

---

## PART 4 — CNN VS TABULAR NN (PRECISE)

Create a comparison section that:
- Uses THIS experiment as evidence
- Shows exactly where CNNs diverge from tabular NNs
- Explains why AFTER flattening they are mathematically identical

Avoid vague statements like “CNNs learn features”.
Instead, demonstrate feature learning numerically.

---

## PART 5 — STUDY GUIDE / RUNBOOK

Generate a section titled:
**“How to Study This Repository”**

It must:
- Tell the reader EXACTLY which file to open first
- Specify the order of files to inspect
- State what question each file answers
- Suggest specific comparisons (e.g., rectangle vs triangle)- Include a verification spreadsheet template with columns: `image | true_label | conv_max_val | relu_nonzero_count | pool_max_val | flat_nonzero_count | logit_0 | logit_1 | softmax_predicted_class | loss`
This should read like a lab manual.

---

## PART 6 — COMPUTATION VERIFICATION (MANDATORY)

Include a walkthrough section titled:
**"Verify Your Understanding: Manual Convolution Calculation"**

It must:
- Pick one convolution operation (e.g., rectangle_1 at position [0,0], filter 0)
- Show the image patch (3×3 extracted from image)
- Show the kernel (3×3)
- Show all 9 element-wise multiplications explicitly (value1 × kernel1, value2 × kernel2, etc.)
- Show the sum of the 9 products
- Read the bias from code or compute it
- Add the bias to the sum: `manual_result = sum_of_products + bias`
- Load the expected value from CSV file (exact path shown)
- Compare: `manual_result` vs `CSV_value`
- Report: Match or mismatch with tolerance (1e-4)

Include a **REQUIRED SELF-CHECK TABLE** with these columns:

| Stage | File Path | Expected Shape | Observed Shape | Min | Max | Sum (if applicable) | Pass/Fail | Notes |
|-------|-----------|---|---|---|---|---|---|---|
| Input | rectangle_1_01_pixels.csv | 8×8 | ? | ? | ? | N/A | ? | ? |
| Conv F0 | rectangle_1_02_conv_f0.csv | 6×6 | ? | ? | ? | N/A | ? | ? |
| ReLU F0 | rectangle_1_03_relu_f0.csv | 6×6 | ? | ? | ? | N/A | ? | ? |
| Pool F0 | rectangle_1_04_pool_f0.csv | 3×3 | ? | ? | ? | N/A | ? | ? |
| Flatten | rectangle_1_05_flat.csv | 1×18 | ? | ? | ? | N/A | ? | ? |
| Logits | rectangle_1_06_logits.csv | 1×2 | ? | ? | ? | N/A | ? | ? |
| Softmax | rectangle_1_07_softmax.csv | 1×2 | ? | ? | ? | **1.0** | ? | ✅ or ❌ |

**For each row:** Fill in observed values from actual CSV. Report Pass only if shape matches AND (if applicable) constraints hold.

This transforms passive reading into active verification and catches errors before they propagate.

---

## PART 7 — INFERENCE ON UNSEEN SHAPES (CONDITIONAL)

**ONLY include this section if BOTH conditions are met:**
1. The Python code generates square and pentagon images
2. The output files `inference/square_1_inference_07_softmax.csv` and `inference/pentagon_1_inference_07_softmax.csv` exist in the repo

If either condition is false, write:
"**Inference on unseen shapes:** Not implemented in this version of the repository. To add this, generate square and pentagon images in the code and re-run."

If both conditions are true, analyze:
1. **One known shape** (e.g., rectangle_1): a shape from the training set
2. **Square** (new): a shape structurally similar to rectangles but not in training data
3. **Pentagon** (new): a shape structurally different from both training shapes

For each inference image:
- Show the softmax probabilities (must sum to 1.0)
- Report which class it predicts
- Analyze WHY (which features activated?)

---

## PART 8 — BIG PICTURE SYNTHESIS

End with:
- A one-paragraph mental model of CNNs

- A bullet list of “things that look new but are not”
- A bullet list of “things that are genuinely new vs tabular NN”

---

# OUTPUT FORMAT REQUIREMENTS

- Output as a single Markdown document
- Use headings and subheadings
- Use code blocks for formulas and code snippets
- Use tables where helpful
- Assume this will be read as course notes or a mini-textbook chapter
- Include file paths relative to repository root using exact filenames (e.g., `rectangle_1_02_conv_f0.csv`, not placeholder names)
- End with a **"Validation Checklist"** section: "Can you answer these 10 core questions? (Yes/No for each)" covering key concepts from all 6 parts

# CONSTRAINTS

- Do NOT generalize beyond this repo. Explicitly: Do NOT mention batch norm, dropout, residual connections, different architectures, pre-trained models, data augmentation, or training loops
- Do NOT introduce concepts not present in this specific codebase (8×8 images, 2 conv filters, 3×3 kernels, single dense layer)
- Do NOT skip steps or combine explanations into vague summaries
- Do NOT simplify at the cost of correctness. Precision over brevity

---

# NON-NEGOTIABLE VALIDATION RULES (MUST FOLLOW)

## Rule 1: Never Fabricate Values
You MUST NOT fabricate values. Every numeric claim (min/max/mean/logits/softmax/loss) MUST be derived from actual repository artifacts:
- The Python code OR
- The generated CSV/TXT files

If a value is not directly available, you must compute it from the files and show the exact file path used.
**If you cannot cite a source, do not state the claim.**

## Rule 2: Validate Shapes at Every Stage
For each stage, explicitly state tensor shape and confirm it matches file dimensions:
- **pixels:** 8×8 (64 values)
- **conv:** 6×6 per filter (36 values per filter, 72 total for 2 filters)
- **relu:** 6×6 per filter (36 values per filter)
- **pool:** 3×3 per filter (9 values per filter, 18 total for 2 filters)
- **flat:** 1×18 (exactly 18 values)
- **logits:** 1×2 (exactly 2 values, one per class)
- **softmax:** 1×2 (exactly 2 values, prob[0] + prob[1] must equal 1.0 ± 1e-6)

If any mismatch occurs, diagnose it and do not proceed. Report the mismatch explicitly.

## Rule 3: Enforce Probability Correctness
- Softmax outputs MUST sum to 1.0 (tolerance: ±1e-6)
- If softmax sum ≠ 1.0, treat it as an error and investigate immediately
- Do NOT report softmax with sum < 1 or > 1

## Rule 4: Keep Normalization Statements Consistent
- If pixel values in CSV are 0–255, do NOT claim normalization to 0–1
- If normalization is performed, show:
  - The code line that performs it
  - The min/max BEFORE normalization
  - The min/max AFTER normalization
- Do NOT use conflicting statements in the same section

## Rule 5: Include Bias in All Computations
- Convolution formula MUST include conv bias term
- Dense layer formula MUST include bias
- Manual convolution verification MUST include bias in the final sum
- If bias values are not available, state this explicitly and note its impact

## Rule 6: Do NOT Invent an Inference Pipeline
Only include "inference on unseen shapes" if:
- The code actually generates those shapes (check the script) AND
- The outputs actually exist in the repo as CSV files (check the folder)

Otherwise:
- Omit inference entirely, OR
- Write explicitly: "Not implemented in this repo yet" and do NOT guess what might happen

## Rule 7: Parameter Counts MUST Match This Exact Model
You MUST compute parameter counts from the actual architecture in code:
```
conv weights: 2 filters × 1 input × 3×3 kernel = 18
conv bias: 2
fc weights: 18 inputs × 2 outputs = 36
fc bias: 2
──────────────────────────────────
TOTAL = 58 parameters
```

Do NOT compare to tabular models with arbitrary hidden sizes without explicit definition.
If you compare to a "dense-only baseline," define it (e.g., "64→2 direct dense layer") and compute params correctly.

---

# EVIDENCE TAGGING REQUIREMENT

Every factual claim MUST include an evidence tag showing its source.

Use one of these formats at the end of the sentence or paragraph:
- `[SOURCE: code: cnn_demo_toy_story_exhaustive.py:L<line>]`
- `[SOURCE: file: cnn_demo_story_outputs_exhaustive/<filename>]`
- `[SOURCE: computed from: <filepath> using <operation>]`

Examples:
- "Convolution output is 6×6 per filter [SOURCE: code: cnn_demo_toy_story_exhaustive.py:L42]"
- "Rectangle_1 has 24 nonzero pixels [SOURCE: computed from: cnn_demo_story_outputs_exhaustive/rectangle_1_01_pixels.csv using count(nonzero)]"
- "The softmax values are [0.9915, 0.0085] [SOURCE: file: cnn_demo_story_outputs_exhaustive/rectangle_1_07_softmax.csv]"

**If you cannot attach a source tag, do not state the claim.**

This is non-negotiable: evidence tags almost completely stop hallucinated numbers.

---



## PART 8 — BIG PICTURE SYNTHESIS (UPDATED)

End with:
- A one-paragraph mental model of CNNs
- A bullet list of "things that look new but are not"
- A bullet list of "things that are genuinely new vs tabular NN"

Include a **FINAL VALIDATION CHECKLIST** with 10 questions covering all PARTS that reader must answer with confidence.

---

# SUCCESS CRITERION

A reader who understands tabular neural networks should be able to:
1. Explain every number produced by this code (with evidence citations)
2. Manually compute at least one convolution dot product, including bias, and verify against a CSV file using the self-check table
3. Trace a single pixel difference from rectangle to triangle through all layers and report shape/value mismatches
4. Describe exactly where CNNs diverge from tabular NNs (before flatten) and why they're identical after
5. Pass the self-check table: all rows show observed shapes matching expected shapes, and softmax sums to 1.0
6. Answer all 10 validation checklist questions with confidence

Final statement of understanding:
"I now understand CNNs, and I can explain every computation at every layer using data from this experiment, backed by evidence tags."

---
